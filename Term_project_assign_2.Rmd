---
output: html_document
editor_options: 
  chunk_output_type: console
---
## Term Project Assignment 2 - 10 pnts

TESTING THAT THIS WORKS

In this assignment you will develop the main goal and objectives of your term project and explore, evaluate, and select data sources. A key component of this process is formulating a research question or develop a workflow. The workflow for this assignment will help you refine your question and identify the datasets needed to address it. 

### Brainstorming

**Q1. What is the primary research question/objective for your term project? (1-2 sentences)(1 pt)** <br>

Think of this as your first approach to developing a research question. It should reflect a clear purpose and be specific enough to guide your initial search for data, but it likely will evolve throughout this assignment. That is okay, even expected. You will be asked for a refined response at the end of the assignment.<br>
Consider: <br>
- The problem or issue you want to address  <br>
- What specific phenomenon within that issue are you interested in understanding  <br>
- What measurable or observable outcomes do you want to analyze.  <br>
For example, instead of 'how does climate change affect forests?', you might consider 'How has seasonal precipitation variability impacted the timing of peak NDVI in the Pacific Northwest from 2000 to 2020.' Note that this question is specific enough to guide your search for the necessary data, such as precipitation records and NDVI time series. <br>

ANSWER: How has variability in stream discharge, precipitation, and air temperature impacted salmon returns in Western Washington.

**Q2. What are 2-3 types of data your research question requires? Address each sub-question (2 pts)**
Be as specific as you can, what types of variables or indicators are you looking for? <br>
What format should the data be in (e.g., spatial datasets, time-series data)? <br>
What might be the origin of this data? (e.g., satellite imagery, weather station records, survey data?) <br>
What temporal length and resolution would be ideal to answer this question?<br>
ANSWER:
Salmon Return Data: survey data
Precipitation: weather station
Stream Temperature: stream gauge
Stream Flow: stream gauge

ANSWER:
1) For this project the variables and indicators we are looking for is salmon return data (specifically from streams with USGS gages), stream discharge data, precipitation data, and air temperature data.

2) The data should all be in the form of time-series data, however, we will likely use the map function previously used in labs to display where the stream gages we are using information from are located. 

3) The salmon return data will be survey type data from the stateofsalmon.wa.gov website. The stream discharge data will come from three different USGS gages. The precipitation and air temperature data will come from NOAA weather station records. 

4) As salmon have a 3-5 year life cycle depending on the species a longer temporal length would be best, ideally spanning 20+ years so that patterns across multiple generations can be examined. The resolution of the data will depend on the variable. Salmon return data are reported as annual totals, so this data will be represented as a single value each year. However, having daily data for stream flow, precipitation, and air temperature would allow for examination of annual patterns and help to determine if there is a correlation between changes in these factors and salmon returns. Therefore these three variables will be examined on a daily basis. 


### Data exploration and selection

Now that you have an idea of what kind of data your question requires, the next step is to locate actual datasets that support your inquiry. There are numerous datasets available to public use, including public repositories, government agencies, academic institutions, even data uploaded to repositories like Zendodo and Figshare making data for specific studies discoverable and citable by other researchers. You may even have datasets from your own research that you can use. (Note: If you plan to use a dataset from your own research, you must have direct access to it now, not just a promise that you can obtain it later. Do not commit to a dataset unless you already have it and can work with it this semester). 
It is up to you to explore a variety of sources and identify datasets that you can retrieve and work with, but here are some ideas and suggestions to get you started. 

Many of these platforms allow R access to their datasets with packages that facilitate downloading, managing, and analyzing the data directly within R. However if you can not find an appropriate package, see below for a webscraping example. <br>

Climate or weather data:<br>
[NOAA](https://www.ncei.noaa.gov/cdo-web/),<br> [SNOTEL](https://www.nrcs.usda.gov/resources/data-and-reports/snow-and-water-interactive-map], NASA POWER[https://power.larc.nasa.gov/data-access-viewer/), <br> and [Google's climate engine](https://www.climateengine.org/) are all potential sources of earth data. <br>

Hydrological data: [USGS](https://waterdata.usgs.gov/nwis/rt) <br>

Soil data: [Web Soil Survey](https://websoilsurvey.nrcs.usda.gov/app/), try R package soilDB <br>

Water quality and air pollution: [EPA Envirofacts](https://enviro.epa.gov/)<br>


Fire: [Monitoring Trends in Burn Severity](https://www.mtbs.gov/) <br>

Remotely sensed spatial data: You may have already some experience with Google Earth Engine. This is a fast way to access [an incredible library](https://developers.google.com/earth-engine/datasets/) of resources. If you have used this in other courses but feel rusty, we can help!  Also check out [this e-book](https://www.eefabook.org/go-to-the-book.html). However, if your interest is in a few images, USGS' Earth Explorer [GloVis](https://glovis.usgs.gov/) is also a good resource. 

#### Expand your search
This is not an exhaustive list of possibilities. Consider searching MSU's library databases. For example, the Web of Science database contains organized searchable datasets built by published researchers. Google searches using advanced search operators to include specific file types with your search terms can be helpful (e.g., .csv, .xlsx, .geojson). Engage with AI tools to explore potential datasets or repositories based on your specific topic. An active approach to searching and learning will help you discover datasets that will support your research question. <br> You can download and import data if the dataset is small, or for large datasets, you might try webscraping. 

Webscraping in R is a method of extracting data from web pages when the data isn't available through an API or a direct download. Check out some of the methods in this 15 min video: [webscraping example for R with ChatGPT](https://youtu.be/MHdMFxUyGdk). There are many similar tutorials, some tailored to specific data types. Take some time to explore the available resources.

#### Adapt your research question
Once you spend a solid hour or two searching databases, you may find that you need to adapt your research question based on data availability. You will likely spend some time refining your initial question to fit the datasets you find and that is encouraged! Then resume the dataset search, and continue refining your question and searching until you can develop an executable methodology. <br>

**Q3. Explore your data, this part requires 2 answers (3 pnts total)**

You are encouraged to explore many packages and write many more code chunks for your personal use. However, for the assignment submission, retrieve at least one dataset relevant to your question using an R package like dataRetrieval or by webscraping in R. If you are using your own data you can start a project with necessary .Rmd, .csv, etc. If you would like to house this as a github repo let me know and I will help you. 

Be sure that your methods are [reproducible](https://guides.lib.uw.edu/research/reproducibility), meaning I should be able to run your code from my computer and reprodduce your workflow. Similarly, try writing required packages using a the function script below. This ensures that if another user does not have a package installed, it will be installed before it is loaded.  

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Install needed packages

WE STILL NEED THIS DONE!!!
```{r}
# #Load packages
# 
# # pkgTest is a helper function to load packages and install packages only when they are not installed yet.
# pkgTest <- function(x)
# {
#   if (x %in% rownames(installed.packages()) == FALSE) {
#     install.packages(x, dependencies= TRUE)
#   }
#   library(x, character.only = TRUE)
# }
# neededPackages <- c("tidyverse", "lubridate", "Evapotranspiration") 

# for (package in neededPackages){pkgTest(package)}

#remotes::install_github("https://github.com/ropensci/rnoaa.git")
#install.packages("daymetr")


library(dataRetrieval)
library(tidyverse)
library(dplyr)
library(patchwork)
library(rnoaa) #add code make it reproduce
library(zoo)
library(daymetr)
library(leaflet)

```

```{r}
#Mackenzies Attempt at above code, all works except trying to cue the git link

# Helper function to load packages and install them if they are not installed yet
pkgTest <- function(x) {
  if (x %in% rownames(installed.packages()) == FALSE) {
    install.packages(x, dependencies = TRUE)
  }
  library(x, character.only = TRUE)
}

# List of needed packages
neededPackages <- c("tidyverse", "lubridate", "Evapotranspiration", "dataRetrieval", "dplyr", "patchwork", "zoo", "daymetr", "leaflet")

# Install and load needed packages
for (package in neededPackages) {
  pkgTest(package)
}


#remotes::install_github("https://github.com/MackenzieAdams1/Salmon-Term-Project.git")
#install.packages("daymetr")

# Load libraries
library(dataRetrieval)
library(tidyverse)
library(dplyr)
library(patchwork)
library(zoo)
library(daymetr)
library(leaflet)

```



```{r}
# With the "help" of Poole (he wrote it), this has been rewritten such that all the stream 
# data is in one df.
# Can talk through the implications of this later. But is very nice

# This efficiently automates the process of getting all the stream data desired into one df.
# This is allowed to be a df w/ minimal info bc it has to be user defined and im not gonna go 
# thru the effort of developing a "frontend" for this

gage_ids <- 
  tribble(
    ~site_name,        ~site_no,
    "Dungeness_Gage",  "12048000",
    "Elwha_Gage",      "12045500",
    "Skokomish_Gage",  "12061500")
  

# set all the parameters
start <- date("1987-10-01")
end <- date("2023-9-30")
para <-c("00060", "00065")

# if sites in gage_ids are changed, uncomment the code below and run it once. This stores the 
# data locally and eventually in the repo if its pushed. This is mostly to be polite to the  
# USGS hosting service.

# NWIS_data <- readNWISdv(gage_ids$site_no, para, start, end)
# saveRDS(NWIS_data, "NWIS_data.rdata")

# NWIS_meta <- readNWISsite(gage_ids$site_no)
# saveRDS(NWIS_meta, "NWIS_meta.rdata")

gage_ids <- 
  readRDS("NWIS_meta.rdata") %>% 
  select(site_no,
         dec_lat_va, 
         dec_long_va) %>%
  inner_join(gage_ids, .)

gage_data <-
  readRDS("NWIS_data.rdata") %>%
  inner_join(gage_ids, .) %>%
  renameNWISColumns() %>%
  group_by(site_name, site_no)


# Shows all unique grouped elements in gage_data. Verifies I got all the desired gages
summarize(gage_data)


```


```{r}
# automate downloading climate data

# write_csv(x = select(gage_ids, -"site_no"), file = "gage_ids.csv")

# daymet_all <- download_daymet_batch(file_location = "./gage_ids.csv", start = year(start), end = year(end), internal = T, simplify = F)
# saveRDS(daymet_all, "DAYMET_all.rdata")


# Downloading data works. Now to append it...

daymet_unfucker_hum <-
  function(f) {
    bind_cols(data = f$data, site = f$site, .name_repair = "unique")
  } 

gage_data_all <- 
  lapply(readRDS("./DAYMET_all.rdata"), daymet_unfucker_hum) %>%
  bind_rows() %>%
  rename(doy = yday, 
         ann = year,
         prcp_daily_mm = prcp..mm.day.,
         t_max_C = tmax..deg.c.,
         t_min_C = tmin..deg.c.) %>%
  mutate(
    Date = make_datetime(year = ann, day = doy),
    t_avg_C = (t_max_C - t_min_C)/2) %>%
  select(site,
         Date,
         prcp_daily_mm,
         t_max_C,
         t_min_C,
         t_avg_C
         ) %>%
  left_join(ungroup(gage_data), ., join_by(site_name == site, Date == Date))

```


```{r}
#Mackenzie
# antipattern format. Can be addressed once the rest of the codebase is rewritten
#This code identifies the wanted gages and parameters, and then reads in info about the gages.

Dungeness_Gage <- "12048000"
Elwha_Gage <- "12045500"
Skokomish_Gage <- "12061500"

start <- date("1987-10-01")
end <- date("2023-9-30")
para <-("00060")

Dungeness_Stream_Data <- readNWISdv(siteNumbers = Dungeness_Gage, parameterCd = para, startDate = start, endDate = end) %>%
  renameNWISColumns()

Elwha_Stream_Data <- readNWISdv(siteNumbers = Elwha_Gage, parameterCd = para, startDate = start, endDate = end) %>%
  renameNWISColumns()

Skokomish_Stream_Data <- readNWISdv(siteNumbers = Skokomish_Gage, parameterCd = para, startDate = start, endDate = end) %>%
  renameNWISColumns()

```

```{r}
#Mackenzie
#This code retrieves the latitude and longitude for each of the sites and then combines them into one data frame that can be used to make a map of all the sites being compared.

Dungeness_Site_Info <- readNWISsite(Dungeness_Gage) 
Dungeness_latitude <- Dungeness_Site_Info$dec_lat_va 
Dungeness_longitude <- Dungeness_Site_Info$dec_long_va

Elwha_Site_Info <- readNWISsite(Elwha_Gage) 
Elwha_latitude <- Elwha_Site_Info$dec_lat_va 
Elwha_longitude <- Elwha_Site_Info$dec_long_va

Skokomish_Site_Info <- readNWISsite(Skokomish_Gage) 
Skokomish_latitude <- Skokomish_Site_Info$dec_lat_va 
Skokomish_longitude <- Skokomish_Site_Info$dec_long_va

site_names <- c("Dungeness_Gage", "Elwha_Gage", "Skokomish_Gage")
latitudes <- c(Dungeness_latitude, Elwha_latitude, Skokomish_latitude)
longitudes <- c(Dungeness_longitude, Elwha_longitude, Skokomish_longitude)

lat_long_df <- data.frame(Site = site_names, Latitude = latitudes, Longitude = longitudes)
```


```{r}
#Mackenzie 
#find a way to make this more streamlined as in that it will generate lat long for a gage if you just put in a gage number

#This code downloads the weather data for Port Angeles, WA, representing the weather at the Elwha and Dungeness gages. It then creates separate data frames for the daily precipitation (mm), TMax temperature, and TMin temperature.

portangeles_weather_df <- download_daymet(
  site = "Port Angeles",
  lat = 48.05478018,
  lon = -123.5832136,
  start = 1987,
  end = 2023,
  internal = TRUE,
  simplify = TRUE # return tidy data !!
  )

portangeles_weather_df <- portangeles_weather_df %>%
  mutate(date = as.Date(paste(year, yday, sep = "-"), "%Y-%j"))

prcp_mm_day_portangeles <- portangeles_weather_df %>%
  filter(measurement == "prcp..mm.day.")

tmax_deg_c_portangeles <- portangeles_weather_df %>%
  filter(measurement == "tmax..deg.c.")

tmin_deg_c_portangeles <- portangeles_weather_df %>%
  filter(measurement == "tmin..deg.c.")

temp_data_pa_df <- merge(tmax_deg_c_portangeles, tmin_deg_c_portangeles, by = "date", suffixes = c("_tmax", "_tmin")) %>% 
  select("date", "value_tmax", "value_tmin")
```

**Write a brief summary (3-4 sentences) describing your data.** 

Generate plots:
Create at least one plot that summarizes the data and describe it's use to you. Are there gaps in the data? Does the data cover the time or space you are interested in? Are there significant outliers that need consideration? 

```{r}
#Mackenzie 
#find a way to make this more streamlined as in that it will generate lat long for a gage if you just put in a gage number

#This code downloads the weather data for Skokomish, WA, representing the weather at the Skokomish gage. It then creates separate data frames for the daily precipitation (mm), TMax temperature, and TMin temperature.

skokomish_weather_df <- download_daymet(
  site = "Skokomish",
  lat = 47.3098151,
  lon = -123.1770995,
  start = 1987,
  end = 2023,
  internal = TRUE,
  simplify = TRUE # return tidy data !!
  )

skokomish_weather_df <- skokomish_weather_df %>%
  mutate(date = as.Date(paste(year, yday, sep = "-"), "%Y-%j"))

prcp_mm_day_skokomish_df <- skokomish_weather_df %>%
  filter(measurement == "prcp..mm.day.")

tmax_deg_c_skokomish_df <- skokomish_weather_df %>%
  filter(measurement == "tmax..deg.c.")

tmin_deg_c_skokomish_df <- skokomish_weather_df %>%
  filter(measurement == "tmin..deg.c.") 

temp_data_sk_df <- merge(tmax_deg_c_skokomish_df, tmin_deg_c_skokomish_df, by = "date", suffixes = c("_tmax", "_tmin")) %>% 
  select("date", "value_tmax", "value_tmin")

```



```{r}
#Mackenzie 
#This code calculates the average daily temperature for the port angeles weather data.

temp_data_pa_df <- temp_data_pa_df %>%
  mutate(avg_pa_temp_df = (value_tmax + value_tmin) / 2)

#This code calculates the average daily temperature for the skokomish weather data.
temp_data_sk_df <- temp_data_sk_df %>%
  mutate(avg_sk_temp_df = (value_tmax + value_tmin) / 2)
```



```{r}
Dungeness <- Dungeness_Stream_Data %>%
   ggplot(aes(x=Date, y= Flow)) + geom_line() + theme_classic() + labs(x = "Date", y = "Discharge (cfs)")

Elwha <- Elwha_Stream_Data %>%
   ggplot(aes(x=Date, y= Flow)) + geom_line() + theme_classic() + labs(x = "Date", y = "Discharge (cfs)")

Skokomish <- Skokomish_Stream_Data %>%
   ggplot(aes(x=Date, y= Flow)) + geom_line() + theme_classic() + labs(x = "Date", y = "Discharge (cfs)")

Dungeness/Elwha/Skokomish
```



```{r}
prcp_mm_day_portangeles_gg <- portangeles_weather_df %>%
  filter(measurement == "prcp..mm.day.") %>%
  ggplot(aes(x = date, y = value)) +
  geom_line(color = "blue") +
  theme_linedraw() +
  labs(y = "Daily Precipitation (mm)", x = "Date")

tmax_deg_c_portangeles_gg <- portangeles_weather_df %>%
  filter(measurement == "tmax..deg.c.") %>%
  ggplot(aes(x = date, y = value)) +
  geom_line(color = "blue") +
  theme_linedraw() +
  labs(y = "Max Temperature (C)", x = "Date")

tmin_deg_c_portangeles_gg <- portangeles_weather_df %>%
  filter(measurement == "tmin..deg.c.") %>%
  ggplot(aes(x = date, y = value)) +
  geom_line(color = "blue") +
  theme_linedraw() +
  labs(y = "Min Temperature (C)", x = "Date")
```



```{r}
prcp_mm_day_skokomish_gg <- skokomish_weather_df %>%
  filter(measurement == "prcp..mm.day.") %>%
  ggplot(aes(x = date, y = value)) +
  geom_line(color = "blue") +
  theme_linedraw() +
  labs(y = "Daily Precipitation (mm)", x = "Date")

tmax_deg_c_skokomish_gg <- skokomish_weather_df %>%
  filter(measurement == "tmax..deg.c.") %>%
  ggplot(aes(x = date, y = value)) +
  geom_line(color = "blue") +
  theme_linedraw() +
  labs(y = "Max Temperature (C)", x = "Date")

tmin_deg_c_skokomish_gg <- skokomish_weather_df %>%
  filter(measurement == "tmin..deg.c.") %>%
  ggplot(aes(x = date, y = value)) +
  geom_line(color = "blue") +
  theme_linedraw() +
  labs(y = "Min Temperature (C)", x = "Date")
```


```{r}
#building a map of the site locations
gage_map <- leaflet() %>%
  addProviderTiles("OpenStreetMap") %>%
  addAwesomeMarkers(data = lat_long_df,lat = ~Latitude, lng = ~Longitude, label = ~Site, 
                    popup = ~paste("Site", Site)) 
  
gage_map

```


Generate a histogram or density plot of at least one variable in your dataset. The script here will help start a density plot showing multiple variables. You may adapt or change this as needed. 

```{r}
#plottable_vars <- dfname %>%
#  dplyr::select(variable1, variable2,...)
                
#long <- plottable_vars %>%
#  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

# Plot density plots for each variable
#ggplot(long, aes(x = Value, fill = Variable)) +
#  geom_density(alpha = 0.5) +  # Add transparency for overlapping densities
# facet_wrap(~ Variable, scales = "free", ncol = 2) +  # Create separate panels per variable
#  theme_minimal() +
#  labs(title = "Density Plots for Variables",
 #      x = "Value", y = "Density")


temp_fish <-
  read.csv("./Dungeness Salmon Data.csv")


gage_fish_all %>%
  ggplot(aes(x = t_avg_C, fill = prcp_daily_mm)) +
  geom_density() +
  facet_wrap(~ site_name)

```


**What does the above plot tell you about the distribution of your data? Is it as expected? Why do we need to consider the distribution of data when deciding on analysis methods?**


**Q4. Putting it all together (3 pts)**
Create a table in R and export it as a .csv file to submit with this assignment .rmd.

In this table, include all datasets (up to now) that you will use for your term project. The script below specifies all of the sections required in this table, though you will need to change names and information accordingly in c(). Also, not everything in this table may pertain to you and your project. So we can change things accordingly. 

```{r, message=FALSE}
read.csv("./all_data_preview.csv")
``` 

Once this is exported, you can format if desired for readibility and submit the .csv with a completed .rmd. 

**Q5. What is your updated term project question/objective? How, specifically, will the data sources listed above help you to answer that question? (1 pt)**

How have fluctuations in stream discharge, precipitation, and air temperature influenced the survivability and return rates of Chinook salmon in Western Washington from 1987 to 2023? The data sources above will help to answer our question by providing additional insights into potential variables impacted salmon runs. Precipitation and stream discharge values will help us to analyze the flow conditions that salmon populations were experiencing within a given year and track the correlating impacts on salmon return numbers. Additionally, air temperature will help us to better understand stream conditions that the salmon were experiencing within a given season and tie these values to low flows. These climatological trends can then we tracked to analyze how variables that salmon experienced during the juvenile stage impact the return numbers three to five years later. 

